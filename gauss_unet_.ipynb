{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `DATA NOISE DISTRIBUTION & FEATURE MAP ANALYSIS`"
      ],
      "metadata": {
        "id": "MbAks5-h4Gle"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2O2YnZfL4MKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `GAUSSUNET`\n",
        "\n",
        "\n",
        "```\n",
        "- 모델을 pypi 패키지화 하기로 한다.\n",
        "- 우선은 모델을 encoder - skip connection - decoder 이렇게 나눠야함\n",
        "- modules.py 파일 안에 모든 layer이나 필요한 module들을 넣어 주어야 함 \n",
        "  - 예를 들면 dense_res_block, res_block, ssa_module, pyramid_attention, scse등\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "f-l2xFSF3crc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenoiserModel(nn.Module):\n",
        "  def __init__(self, \n",
        "               head,\n",
        "               encoder,\n",
        "               decoder,\n",
        "               tail):\n",
        "    super(DenoiserModel, self).__init__()\n",
        "    self.head = head\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tail = tail\n",
        "\n",
        "  def forward(self, x):\n",
        "    input = x.clone()\n",
        "    head = self.head(x)\n",
        "    features = self.encoder(head)\n",
        "    ## decoder의 경우에는 encoder의 각 layer의 출력 feature map을 사용해서 최종 decoder output을 생성\n",
        "    output = self.decoder(*features)\n",
        "    out = self.tail(output, input)\n",
        "\n",
        "    return out\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    model.predict(8F RAW IMAGE)를 하면 알아서 denoised 32F SYN IMAGE를 출력할 수 있게 한다.\n",
        "    \"\"\"\n",
        "    if len(x.shape) == 3:\n",
        "      x = x.unsqueeze(0)\n",
        "    \n",
        "    self.eval()\n",
        "    out = self.forward(x)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "quv-dQ7DGOMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_head(head_params):\n",
        "  return HeadBlock(**head_params)\n",
        "\n",
        "def build_encoder(encoder_params):\n",
        "  return Encoder(**encoder_params)\n",
        "\n",
        "def build_decoder(decoder_params):\n",
        "  return Decoder(**decoder_params)\n",
        "\n",
        "def build_tail(tail_params):\n",
        "  return TailBlock(**tail_params)"
      ],
      "metadata": {
        "id": "JhzoS8FNKWhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `MODULES`"
      ],
      "metadata": {
        "id": "XXxVZ_KG_kQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "base_modules.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "############# HEAD & TAIL BLOCKS #####################\n",
        "##### (1) HEAD BLOCK\n",
        "class HeadBlock(nn.Module):\n",
        "  def __init__(self, in_channels = 1, out_channels = 8):\n",
        "    super(HeadBlock, self).__init__()\n",
        "    self.head = nn.Sequential(\n",
        "        nn.BatchNorm2d(in_channels), nn.Tanh(),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 0, stride = 1)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.head(x)\n",
        "\n",
        "##### (2) TAIL BLOCK\n",
        "class TailBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, tail_rate = 0.1):\n",
        "    super(TailBlock, self).__init__()\n",
        "    self.tail = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1, stride = 1)\n",
        "    self.tail_rate = tail_rate\n",
        "\n",
        "  def forward(self, x, input):\n",
        "    x = self.tail(x)\n",
        "    out = input + (x * self.tail_rate)\n",
        "\n",
        "    return torch.clamp(out, -2, 2)\n",
        "\n",
        "############# EXTRACTION BLOCKS ######################\n",
        "### (1) NOISE EXTRACTION BLOCK\n",
        "### TODO: MUST BE FIXED AFTER ANALYZING THE NOISE LEVELS OF THE FEATURE MAPS\n",
        "class NoiseExtractionBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(NoiseExtractionBlock, self).__init()\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 0, stride = 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.block(x)\n",
        "\n",
        "### (2) FEATURE EXTRACTION BLOCK ####\n",
        "class FeatureExtractionBlock(nn.Module):\n",
        "  def __init__(self, name,in_channels, out_channels):\n",
        "    super(FeatureExtractionBlock, self).__init__()\n",
        "    self.name = name.lower()\n",
        "    if name.lower() == 'conv1':\n",
        "      ## convolution 안에서는 downsample이나 upsampling을 안한다는 가정이다.\n",
        "      self.block = nn.Conv2d(in_channels,out_channels, kernel_size = 1, padding = 0, stride = 1)\n",
        "\n",
        "    elif name.lower() == 'conv3':\n",
        "      self.block = nn.Conv2d(in_channels, out_channels,kernel_size = 3, padding = 1, stride = 1)\n",
        "\n",
        "    elif name.lower() == 'sobel':\n",
        "      self.xfilter = nn.Parameter(\n",
        "          torch.Tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]), requires_grad = False)\n",
        "      self.yfilter = nn.Parameter(\n",
        "          torch.Tensor([[-1, 2, -1], [0, 0, 0], [1, 2, 1]]), requires_grad = False)\n",
        "      \n",
        "    elif name.lower() == 'laplacian':\n",
        "      ## 원래 laplacian filter에서의 중심 픽셀은 4인데 (왜냐면 픽셀의 합이 0이어야 해서) 하지만 2로 하는게 더 나았음\n",
        "      ## 일반적으로는 gaussian filtering을 한 이후에 하지만\n",
        "      self.filter = nn.Parameter(\n",
        "          torch.Tensor([[0, 1, 0], [1, -3, 1], [0, 1, 0]]), requires_grad = False)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    if self.name == 'sobel':\n",
        "      X = F.conv2d(x, self.xfilter)\n",
        "      Y = F.conv2d(x, self.yfilter)\n",
        "      return x + X + Y\n",
        "\n",
        "    elif self.name == 'laplacian':\n",
        "      edge = F.conv2d(x, self.filter)\n",
        "      return x + edge\n",
        "\n",
        "    else:\n",
        "      return self.block(x)\n",
        "\n",
        "\n",
        "###################### CONVOLUTION BLOCK #################################\n",
        "########### ASEBLOCK FOR EXPERIMENT 1 ######################\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, \n",
        "                  feature_ext = 'conv1', noise_ext = None, attention = None):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    \"\"\" Basic Convolution Block for the Decoder Module\n",
        "    - 기본적인 conv1, conv2, conv3로 구성된건 기존의 GaussNet에서의 구조와 동일함\n",
        "    - Feature Extraction Block은 residual 하게 무조건 feature preservance를 위해서 필요\n",
        "    - Noise Extraction Block은 noise를 extract하기 위해서 사용할 예정인데, 그러기 위해서는 noise level map에 대한 분석이 필요하다.\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(in_channels, affine = True), nn.Tanh(),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias = False),\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(out_channels, affine = True), nn.Tanh(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 3, bias = False, padding = 1, stride =1)\n",
        "    )\n",
        "    self.attention = Attention(attention, out_channels)\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(out_channels, affine = True), nn.Tanh(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 1)\n",
        "    )\n",
        "\n",
        "    self.feat_ext = FeatureExtractionBlock(name = feature_ext, in_channels = in_channels, out_channels = out_channels) \n",
        "    self.noise_ext = NoiseExtractionBlock( in_channels = in_channels, out_channels = out_channels) if noise_ext is not False else None\n",
        "    self.final_act = nn.Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    inp = x.clone()\n",
        "    x = self.conv3(self.attention(self.conv2(self.conv1(x))))\n",
        "\n",
        "    identity = self.feat_ext(inp)\n",
        "    if self.noise_ext:\n",
        "      noise = self.noise_ext(inp)\n",
        "      x -= noise\n",
        "    return self.final_act(x + identity)\n",
        "    \n",
        "    \n",
        "\n",
        "#### SPATIAL & CHANNEL SQUEEZE MODULE \n",
        "class SCSEModule(nn.Module):\n",
        "  def __init__(self, in_channels, reduction = 16):\n",
        "    super(SCSEModule, self).__init__()\n",
        "    self.channel = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Conv2d(in_channels, in_channels // reduction, kernel_size = 1),\n",
        "        nn.Tanh(),\n",
        "        nn.Conv2d(in_channels // reduction, in_channels, kernel_size = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.spatial = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 1, kernel_size = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    ch = self.channel(x)\n",
        "    sp = self.spatial(x)\n",
        "    out = ch * x + sp * x\n",
        "    return out\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  ## Attention Module used in the Simple blocks\n",
        "  def __init__(self, name, in_channels):\n",
        "    super(Attention, self).__init__()\n",
        "    if name is None:\n",
        "      self.attention = nn.Identity()\n",
        "    elif name.lower() == 'scse':\n",
        "      self.attention = SCSEModule(in_channels)\n",
        "  def forward(self, x):\n",
        "    return self.attention(x)\n"
      ],
      "metadata": {
        "id": "ENPFIHrBCIFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat = nn.AdaptiveAvgPool2d(1)(torch.rand((2, 32,64,64)))\n",
        "print(feat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1K8wcZ6bzYi",
        "outputId": "eb5d59ec-cdea-4ded-f2c3-9af827f4958e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 32, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "skip_modules.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "### FOR EXPERIMENT 1 ####################\n",
        "#### (1) FiLM BASED SKIP CONNECTION\n",
        "class ConvFiLM(nn.Module):\n",
        "  def __init__(self, in_channels, film_channels):\n",
        "    super(ConvFiLM, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, film_channels, kernel_size = 1)\n",
        "  \n",
        "  def forward(self, skip):\n",
        "    features = self.conv(skip)\n",
        "    features = F.normalize(features, p = 2, dim = -1)\n",
        "    gamma, beta = torch.chunk(features, chunks = 2, dim = 1)\n",
        "\n",
        "    return gamma, beta\n",
        "\n",
        "class LinearFiLM(nn.Module):\n",
        "  def __init__(self, in_channels, film_channels):\n",
        "    super(LinearFiLM, self).__init__()\n",
        "    self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc = nn.Linear(in_channels, film_channels)\n",
        "  \n",
        "    self.film_channels = film_channels\n",
        "\n",
        "  def forward(self, skip):\n",
        "    features = self.pool(skip)\n",
        "    B, C, _, _ = features.shape\n",
        "    features = features.view(B, C)\n",
        "    features = self.fc(features)\n",
        "\n",
        "    features = F.normalize(features, p = 2, dim = -1)\n",
        "\n",
        "    gamma, beta = torch.chunk(features, chunks = 2, dim = 1)\n",
        "    gamma = gamma.view(B, self.film_channels//2, 1, 1)\n",
        "    beta = beta.view(B, self.film_channels//2, 1, 1)\n",
        "\n",
        "    return gamma, beta\n",
        "\n",
        "class FiLMSkipConnection(nn.Module):\n",
        "  def __init__(self, skip_channels, in_channels, film = 'conv'):\n",
        "    super(FiLMSkipConnection, self).__init__()\n",
        "    if film == 'conv':\n",
        "      self.film = ConvFiLM(skip_channels, in_channels * 2)\n",
        "    else:\n",
        "      self.film = LinearFiLM(skip_channels, in_channels*2)\n",
        "\n",
        "  def forward(self, x, skip):\n",
        "    x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "    gamma, beta = self.film(skip)\n",
        "    x = gamma * x + beta\n",
        "    return torch.cat([x, skip], dim = 1)\n",
        "\n",
        "\n",
        "\n",
        "#### (2) SIMPLE SKIP CONNECTION (Just Concat)\n",
        "class SimpleSkipConnection(nn.Module):\n",
        "  def forward(self, x, skip):\n",
        "    x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "    return torch.cat([x, skip], dim = 1)\n",
        "\n",
        "\n",
        "#### (3) SUB SPACE ATTENTION SKIP CONNECTION \n",
        "class SimpleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(SimpleConv, self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(in_channels), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1, stride = 1)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(in_channels), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1, stride = 1)\n",
        "    )\n",
        "    self.identity = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv2(self.conv1(x))\n",
        "    x = self.identity(x)\n",
        "\n",
        "    return out + x\n",
        "\n",
        "class SubNet(nn.Module):\n",
        "  ## SubNet in the SSA Module (Gets the concat of x1[bridge] and x2[upsampled] as input)\n",
        "  def __init__(self, ch_in, layer_n):\n",
        "    super(SubNet, self).__init__()\n",
        "    self.block = nn.ModuleList([SimpleConv(ch_in, ch_in) for _ in range(layer_n)])\n",
        "  \n",
        "  def forward(self, x):\n",
        "    for idx, b in enumerate(self.block):\n",
        "      x = b(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class SSASkipConnection(nn.Module):\n",
        "  def __init__(self, in_channels, skip_channels, conv_layer_n, subspace_dim = 16):\n",
        "    super(SSASkipConnection, self).__init__()\n",
        "    \"\"\"\n",
        "    - Encoder Decoder의 skip connection에서 사용이 되는 SSA의 subspace_dim은 32일 경우 모델이 너무 복잡해져 학습이 잘 되지 않는다.\n",
        "    - 또한, subspace_dim이 늘어나면 over smoothing이 너무 강하게 되는 경향을 보인다.\n",
        "    - SSA에 입력되는 <bridge>는 ConvBlock을 여러개 연결한 Skip Connection을 거치게 된다.\n",
        "    \"\"\"\n",
        "    self.sub_dim = subspace_dim\n",
        "    self.subnet = SubNet(skip_channels, conv_layer_n)\n",
        "    self.conv_block = SimpleConv(in_channels + skip_channels, subspace_dim)\n",
        "\n",
        "  def forward(self, x, skip):\n",
        "    # x : UpScaled Feature Map\n",
        "    # bridge = self.skip_conv(bridge)\n",
        "    x = F.interpolate(x, scale_factor = 2, mode = 'nearest') ## 아직 upscaling을 하지 않았으니 이미지의 크기를 키워준다.\n",
        "    \n",
        "    B, C, H, W = skip.shape\n",
        "    skip = self.subnet(skip)\n",
        "    #print(skip.shape, x.shape)\n",
        "    ## SSA Module Starts ##\n",
        "    # (1) Concat\n",
        "    out = torch.cat([skip, x], dim = 1) ## 처음에 concatenate를 한번 하고\n",
        "    # (2) Sub Space Block (=Conv-Block)\n",
        "    sub = self.conv_block(out)\n",
        "    # (3) Basis Vectors\n",
        "    V_t = sub.reshape(B, self.sub_dim, H*W)\n",
        "    # (4) Projection\n",
        "    V_t = V_t / (1e-6 + torch.abs(V_t).sum(axis = 2, keepdims = True))  # 여기서는 torch.abs()이기 때문에 양수 + 매우 작은 양의 실수라 0이 될 위험은 없다\n",
        "\n",
        "    V = torch.transpose(V_t, 1, 2)\n",
        "    ## transpose를 시키고 원래 matrix와 matmul을 계산하면 대각선 값은 모두 동일함\n",
        "    mat = torch.matmul(V_t, V) ## (B, 16, 16) : 16은 subspace dim\n",
        "    # mat의 determinant를 계산한 결과가 양수가 되도록 abs 함수를 취해주면 된다.\n",
        "    det = torch.clamp(torch.abs(torch.linalg.det(mat)), min = 1e-6)\n",
        "    ## zero-division error을 막기 위해 어떤 수를 더해주는 것이 제일 나을지 모르겠음 (원래는 det에 어떤 작은 실수를 더해서 div가 가능하도록 했었다)\n",
        "\n",
        "    mat_inv = torch.div(mat.permute(2, 1, 0), det).permute(2, 0, 1)\n",
        "\n",
        "    proj_mat = torch.matmul(mat_inv, V_t) ## 그냥 matmul을 하는 수가 1이되도록 하면 loss == NaN이 되는 것은 당연히 막을 수 있다. (V_t)\n",
        "    skip_ = skip.reshape(B, C, W*H)\n",
        "    proj_feat = torch.matmul(proj_mat, torch.transpose(skip_, 1, 2))\n",
        "    skip = torch.matmul(V, proj_feat)\n",
        "    skip = torch.transpose(skip, 1, 2).reshape(B, C, H, W)\n",
        "    \n",
        "    out = torch.cat([x, skip], 1) ## 변화가 된, 즉 subspace projection에 의해서 새로운 latent space로 mapping이 된 skip feature map을 upscaling된 input에 더해준다.\n",
        "    #print(out.shape, \"OUT\")\n",
        "    return out \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tc7BlDTipHA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `ENCODER`"
      ],
      "metadata": {
        "id": "hu3LPZu07skG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained Encoder을 사용하고, 그 주어진 feature map의 크기에 맞게 decoder이 upsampling을 한다.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "###### ENCODER BLOCK ##################\n",
        "#######################################\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, \n",
        "               in_channels,\n",
        "               out_channels,\n",
        "               feature_ext = 'conv1',\n",
        "               noise_ext = False):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(in_channels), nn.Tanh(),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1, stride = 1)\n",
        "    )\n",
        "    self.attention1 = Attention('scse', out_channels)\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(out_channels), nn.Tanh(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 1, padding = 0, stride = 1)\n",
        "    )\n",
        "    self.attention2 = Attention('scse', out_channels)\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.InstanceNorm2d(out_channels), nn.Tanh(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1, stride = 1)\n",
        "    )\n",
        "    self.attention3 = Attention('scse', out_channels)\n",
        "\n",
        "    self.feat_ext = FeatureExtractionBlock(feature_ext, in_channels, out_channels)\n",
        "    self.noise_ext = NoiseExtractionBlock(in_channels, out_channels) if noise_ext else None\n",
        "    self.final_act = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    inp = x.clone()\n",
        "    x = self.attention1(self.conv1(x))\n",
        "    x = self.attention2(self.conv2(x))\n",
        "    x = self.attention3(self.conv3(x))\n",
        "    feat = self.feat_ext(inp)\n",
        "    if self.noise_ext:\n",
        "      noise = self.noise_ext(inp)\n",
        "      x -= noise\n",
        "    x += feat\n",
        "    x = F.interpolate(x, scale_factor = 0.5, mode = 'nearest')\n",
        "\n",
        "    return x\n",
        "\n",
        "###############################################\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, encoder_channels = [8, 16, 32, 64], layer_n = 4):\n",
        "    super(Encoder, self).__init__()\n",
        "    blocks = [\n",
        "        EncoderBlock(ch_in, ch_out) for (ch_in , ch_out) in zip(encoder_channels, encoder_channels[1:])\n",
        "    ]\n",
        "  \n",
        "    self.blocks = nn.ModuleList(blocks)\n",
        "  def forward(self, x):\n",
        "    features = []\n",
        "    for idx, block in enumerate(self.blocks):\n",
        "      features.append(x)\n",
        "      x = block(x)\n",
        "    features.append(x)\n",
        "    features.append(F.interpolate(x, scale_factor = 0.5, mode = 'nearest'))\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "2fRrmlDW7uWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "## HEAD는 (1 -> 8의 channel크기를 갖도록 한다.)\n",
        "x = torch.rand((2, 8, 512, 512))\n",
        "out = encoder(x)\n",
        "for o in out:\n",
        "  print(o.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxbZS5IBoubp",
        "outputId": "62fbe390-1ff0-4d7e-9d94-24e45bcd453e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 512, 512])\n",
            "torch.Size([2, 16, 256, 256])\n",
            "torch.Size([2, 32, 128, 128])\n",
            "torch.Size([2, 64, 64, 64])\n",
            "torch.Size([2, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `DECODER BLOCK`"
      ],
      "metadata": {
        "id": "Ra6yZlLG7qRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##### DECODER BLOCK ###############\n",
        "###################################\n",
        "def build_skipconnection(skip_name, ch_in, skip_ch, conv_n):\n",
        "  skip_name = skip_name.lower()\n",
        "  if skip_name == 'concat':\n",
        "    return SimpleSkipConnection()\n",
        "  elif skip_name == 'film':\n",
        "    return FiLMSkipConnection(skip_ch, ch_in)\n",
        "  elif skip_name == 'ssa':\n",
        "    return SSASkipConnection(ch_in, skip_ch, conv_layer_n =  conv_n)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(\n",
        "      self, \n",
        "      in_channels, ## skip connection을 거치고 나면 decoder layer의 input channel의 크기\n",
        "      skip_channels,\n",
        "      out_channels, ## 최종 output channel의 크기\n",
        "      feat_ext_mode, ## feature extraction mode를 어떻게 할지 (default는 그냥 1x1 convolution)\n",
        "      noise_ext, ## boolean (Noise Extraction을 할지 말지)\n",
        "      attention_mode, ## [\"None\", \"scse\"]\n",
        "      skip_mode, ## [\"concat\", \"ssa\", \"film\"]\n",
        "      skip_conv_n ## dda connection에서 사용\n",
        "    ):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.skip_connection = build_skipconnection(skip_mode, in_channels, skip_channels, skip_conv_n)\n",
        "    self.body = ConvBlock(in_channels + skip_channels, out_channels, feature_ext = feat_ext_mode, noise_ext = noise_ext)\n",
        "\n",
        "\n",
        "  def forward(self, x, skip):\n",
        "    ## skip connection 안에서 upsamping을 해 줌\n",
        "    x = self.skip_connection(x, skip)\n",
        "    x = self.body(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    encoder_channels=[8, 16, 32, 64, 128], \n",
        "    decoder_channels=[128, 64, 32, 16, 8, 1],\n",
        "    feat_ext_mode='conv1',\n",
        "    noise_ext=False,\n",
        "    attention_mode='scse',\n",
        "    skip_mode= 'ssa', # 'film', # 'ssa'\n",
        "    n_blocks=5,\n",
        "    ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.skip_conv_nums = [int(i) for i in range(n_blocks, 0, -1)]\n",
        "    self.encoder_channels = encoder_channels[:n_blocks]\n",
        "    self.decoder_channels = decoder_channels\n",
        "\n",
        "    self.center = nn.Sequential(\n",
        "        nn.InstanceNorm2d(encoder_channels[-2]), nn.Tanh(),\n",
        "        nn.Conv2d(encoder_channels[-2], encoder_channels[-1], kernel_size = 3, padding = 1, stride = 1),\n",
        "        nn.InstanceNorm2d(encoder_channels[-1]), nn.Tanh(),\n",
        "        nn.Conv2d(encoder_channels[-1], decoder_channels[0], kernel_size = 3, padding = 1, stride = 1)\n",
        "    )\n",
        "\n",
        "    blocks = [\n",
        "        DecoderBlock(ch_in, ch_skip, ch_out, feat_ext_mode = feat_ext_mode, noise_ext = noise_ext, attention_mode = attention_mode, skip_mode = skip_mode, skip_conv_n = skip_n) for \\\n",
        "        (ch_in, ch_skip, ch_out, skip_n) in zip(self.decoder_channels, self.encoder_channels[::-1][1:], self.decoder_channels[1:], self.skip_conv_nums)\n",
        "    ]\n",
        "    self.blocks = nn.ModuleList(blocks)\n",
        "  \n",
        "  def forward(self, x, features):\n",
        "    x = self.center(x)\n",
        "    features = features[::-1]\n",
        "    for idx, block in enumerate(self.blocks):\n",
        "      # print(x.shape, features[idx].shape)\n",
        "      x = block(x, features[idx])\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6AkGxmL8KoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((2, 8, 512,512))\n",
        "features = [\n",
        "    torch.rand((2, 8, 512, 512)),\n",
        "    torch.rand((2, 16, 256, 256)),\n",
        "    torch.rand((2, 32, 128, 128)),\n",
        "    torch.rand((2, 64, 64, 64)),\n",
        "]\n",
        "encoder = Encoder()\n",
        "features = encoder(x)\n",
        "for f in features:\n",
        "  print(f.shape)\n",
        "decoder = Decoder()\n",
        "print(decoder(features[-1], features[:-1]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvDORLb_XkD1",
        "outputId": "4d8cec0e-b3d2-4f70-cc1d-cc5dd08d11ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 512, 512])\n",
            "torch.Size([2, 16, 256, 256])\n",
            "torch.Size([2, 32, 128, 128])\n",
            "torch.Size([2, 64, 64, 64])\n",
            "torch.Size([2, 64, 32, 32])\n",
            "torch.Size([2, 8, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3]\n",
        "a[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2LsxnamsOAn",
        "outputId": "8b74c50c-e3d9-4d0b-d51e-5975f83cdb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `SETUP`"
      ],
      "metadata": {
        "id": "kPnplOOJ8zZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setuptools import setup\n",
        "\n",
        "setup(\n",
        "    name = 'gaussunet',\n",
        "    packages = ['gaussunet'],\n",
        "    version = '0.0.1.dev'\n",
        ")\n",
        "\n",
        "# 이제 project root directory에서 pip install -e라고 입력하면 실행 가능한 gaussunet 폴더가 생성이 된다."
      ],
      "metadata": {
        "id": "WfKJHDiA81UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `SMP UNET & UNET++ TEST`"
      ],
      "metadata": {
        "id": "MSsfSxPa4k4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj-mXq9110pt"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation_models_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "unet = smp.Unet(\n",
        "    encoder_name = ''\n",
        ")"
      ],
      "metadata": {
        "id": "k-0qSwwK3gjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}